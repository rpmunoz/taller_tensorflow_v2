{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificación de imágenes usando una red profunda\n",
    "\n",
    "**Profesor:** Roberto Muñoz <br />\n",
    "**E-mail:** <rmunoz@metricarts.com> <br />\n",
    "\n",
    "**Colaborador:** Sebastián Arpón <br />\n",
    "**E-mail:** <rmunoz@metricarts.com> <br />\n",
    "\n",
    "\n",
    "Usaremos el dataset de perros y gatos. El dataset completo se puede descargar desde el link https://www.kaggle.com/c/dogs-vs-cats/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import requests\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descargamos un dataset pequeño de perros y gatos. Lo descargamos ejecutando el siguiente código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = \"https://metriclearning.blob.core.windows.net/tensorflow/cats_and_dogs_small.zip\"\n",
    "\n",
    "# En caso de querer descargar un dataset un poco mas grande\n",
    "# use este link https://metriclearning.blob.core.windows.net/tensorflow/cats_and_dogs_medium.zip\n",
    "\n",
    "data_dir = \"data\"\n",
    "data_file = os.path.join(data_dir, \"cats_and_dogs_small.zip\")\n",
    "\n",
    "if not os.path.exists(data_dir):\n",
    "    os.mkdir(data_dir)\n",
    "\n",
    "response = requests.get(data_url)\n",
    "response_data = response.content\n",
    "\n",
    "with open(data_file, 'wb') as f:\n",
    "    f.write(response_data)\n",
    "\n",
    "with open(data_file, 'rb') as f:\n",
    "    zf = zipfile.ZipFile(f)\n",
    "    zf.extractall(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hacemos un resize de las imágenes a 150 x 150 pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_dir(directory):\n",
    "    cats = glob.glob(os.path.join(directory,\"cats\") + '/*.jpg')\n",
    "    dogs = glob.glob(os.path.join(directory,\"dogs\") + '/*.jpg')\n",
    "    m_images = cats + dogs\n",
    "    \n",
    "    m_labels = []\n",
    "    m_labels.extend([CAT] * len(cats))\n",
    "    m_labels.extend([DOG] * len(dogs))\n",
    "    assert len(m_labels) == len(m_images)\n",
    "    LABELS_DIMENSIONS = 2\n",
    "    m_labels = tf.one_hot(m_labels, LABELS_DIMENSIONS)\n",
    "    print(\"Encontre %d imagenes y etiquetas en %s\" %(len(m_images),directory))\n",
    "    return m_images, m_labels\n",
    "\n",
    "def load_image(path_to_image, p_label):\n",
    "    m_label = p_label\n",
    "    m_image = tf.read_file(path_to_image)\n",
    "    m_image = tf.image.decode_jpeg(m_image)\n",
    "    m_image = tf.image.resize_images(m_image,(150,150))\n",
    "    m_image = m_image / 255\n",
    "    return m_image, m_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"data/cats_and_dogs_small\"\n",
    "train_dir = os.path.join(data_dir, \"train\")\n",
    "val_dir = os.path.join(data_dir , \"validation\")\n",
    "\n",
    "CAT = 0\n",
    "DOG = 1\n",
    "\n",
    "print(\"Carpeta con imagenes para el entrenamiento: \", train_dir)\n",
    "print(\"Carpeta con imagenes para la validación: \", val_dir)\n",
    "print()\n",
    "\n",
    "train_images, train_labels = read_dir(train_dir)\n",
    "val_images, val_labels = read_dir(val_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=np.random.randint(len(train_images))\n",
    "\n",
    "img, label = load_image(train_images[i],\"\")\n",
    "\n",
    "print(type(img))\n",
    "print(type(label))\n",
    "\n",
    "plt.imshow(img)\n",
    "print(train_labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "buffer_size = 1000\n",
    "\n",
    "train_data_set = tf.data.Dataset.from_tensor_slices((train_images,train_labels))\n",
    "train_data_set = train_data_set.shuffle(buffer_size).map(load_image).batch(batch_size)\n",
    "\n",
    "val_data_set = tf.data.Dataset.from_tensor_slices((val_images,val_labels))\n",
    "val_data_set = val_data_set.shuffle(buffer_size).map(load_image).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (x,y) in enumerate(train_data_set):\n",
    "    print(i)\n",
    "    print(x.shape)\n",
    "    print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definimos la arquitectura de la red\n",
    "\n",
    "Usamos el AdamOptimizer como funcion de optimización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_model():\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "\n",
    "    model.add(tf.layers.Conv2D(16, (5,5), activation=tf.nn.relu, input_shape=(150,150,3)))\n",
    "\n",
    "    model.add(tf.layers.MaxPooling2D(pool_size=(2,2),strides=2))\n",
    "\n",
    "    model.add(tf.layers.Conv2D(64,(3,3),activation=tf.nn.relu))\n",
    "    model.add(tf.layers.MaxPooling2D(pool_size=(2,2),strides=2))\n",
    "    model.add(tf.layers.Conv2D(128,(3,3),activation=tf.nn.relu))\n",
    "    model.add(tf.layers.MaxPooling2D(pool_size=(2,2),strides=2))\n",
    "    model.add(tf.layers.Conv2D(128,(3,3),activation=tf.nn.relu))\n",
    "    model.add(tf.layers.MaxPooling2D(pool_size=(2,2),strides=2))\n",
    "    model.add(tf.layers.Flatten())\n",
    "    model.add(tf.layers.Dense(512,activation=tf.nn.relu))\n",
    "    model.add(tf.layers.Dense(2,activation=tf.nn.softmax))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                  optimizer=tf.train.AdamOptimizer(),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "    \n",
    "model = create_model()    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "batch_loss_list=[]\n",
    "val_loss_list=[]\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for (batch, (images, labels)) in enumerate(train_data_set):\n",
    "        batch_loss, batch_accuracy = model.train_on_batch(images.numpy(), labels.numpy())\n",
    "        batch_loss_list.append(float(batch_loss))\n",
    "        \n",
    "        for (dummy, (val_im, val_lab)) in enumerate(val_data_set):\n",
    "            val_loss, val_accuracy = model.evaluate(val_im.numpy(), val_lab.numpy())\n",
    "            val_loss_list.append(float(val_loss))\n",
    "            \n",
    "        if batch%5 == 0:\n",
    "            print('Entrenamiento Epoca #%d\\t Loss: %.6f\\t Accuracy:  %.6f\\t'% (epoch+1, batch_loss, batch_accuracy))\n",
    "            print('Validacion Epoca #%d\\t Loss: %.6f\\t Accuracy:  %.6f\\t' % (epoch+1, val_loss, val_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graficamos la evolución de la función de costo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(8,6))\n",
    "\n",
    "ax.plot(batch_loss_list, label=\"train\")\n",
    "ax.plot(val_loss_list, label=\"validation\")\n",
    "\n",
    "plt.xlabel(\"iteración\")\n",
    "plt.ylabel(\"Función de costo - train\")\n",
    "plt.legend()\n",
    "\n",
    "plt.title(\"Tain dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
